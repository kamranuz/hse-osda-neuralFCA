{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "We choose option Neural FCA. We choose following tabular datasets and targets"
      ],
      "metadata": {
        "id": "FBjzbAabEKns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **№** | **Dataset name**                                                                      | **Description**                        | **Size**     | **Target**     |\n",
        "|----------|--------------------------------------------------------------------------------------------|---------------------------------------------|-------------------|---------------------|\n",
        "| 1        | [Palmer penguins](https://archive.ics.uci.edu/dataset/690/palmer+penguins-3)         | An introductory dataset presented as an <br>alternative to Iris and useful for teaching data <br>exploration/visualization. Data comes from 3 penguin  <br> species in the islands of Palmer Archipelago, Antarctica.   | (344, 9)    | species<br>(3 possible values)             |                                                         |   | ~                 | ~                   |\n",
        "| 2        | [Credit scoring](https://www.kaggle.com/competitions/fintech-credit-scoring/overview) | Bank applicants’ personal data and the fact<br> of default dataset. | (181000, 19) | default flag<br>(2 possible values)       |\n",
        "| 3        | [Zoo](https://archive.ics.uci.edu/dataset/111/zoo)                                    | A simple database containing 17 Boolean-valued<br>attributes of 101 different animals.    | (101, 17)    | type <br>(7 possible values)             |\n",
        "| 4        | [The Complete Pokemon Dataset](https://www.kaggle.com/datasets/rounakbanik/pokemon)                   | Dataset of all 802 Pokemon from all <br>Seven Generations of Pokemon.      | (802,41)                                                                             | legendary flag<br>(2 possible values)                          |"
      ],
      "metadata": {
        "id": "iDm1PrpDB4vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motivation behind our choice is following. **Palmer penguins** is well known benchmark dataset, which is useful for comparisons of different models. **Credit scoring** is close to real world medium sized dataset, which is useful to for scalability checks and performance checks on noisy data. **Zoo** describes structure of real animals relations, which is similar to concept description. In contrast, **The Complete Pokemon Dataset** describes imaginary creatures relations. It is interesting to compare real world system of relations with man made system of relations."
      ],
      "metadata": {
        "id": "2tnCAXqsERHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "We binarize all datasets’ features and targets. The\n",
        "description of preprocessing for every dataset is in the following table"
      ],
      "metadata": {
        "id": "8_0e48vEH5nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| № | Dataset name | Preprocessing Description | Final size | Target |\n",
        "|---|--------------|---------------------------|------------|--------|\n",
        "| 1 | [Palmer penguins (PP)](https://archive.ics.uci.edu/dataset/690/palmer+penguins-3) | We drop 'rowid' column. We drop rows '3' and '271' because all features of the rows are NaN.<br>We use ohe encoding on categorical features ['island','year','sex'].<br>We discretize and perform ohe encoding with continuous features ['bill\\_length\\_mm', 'bill\\_depth\\_mm', 'flipper\\_length\\_mm', 'body\\_mass\\_g'].<br>We use features ['bill\\_length\\_mm', 'bill\\_depth\\_mm','flipper\\_length\\_mm', 'body\\_mass\\_g'].<br>We use quantile-based discretization function.<br>We divide values into 3 quantiles.<br>We impute missing values with KNNimputer. | (342, 19) | Chinstrap species flag |\n",
        "|   |   |   |   |   |\n",
        "| 2 | [Credit scoring (C)](https://www.kaggle.com/competitions/fintech-credit-scoring/overview) | Because of the small number of missing values and the big size of the dataset, we drop all rows with missing values.<br>Because our computation capabilities are limited, we leave the first 90,000 rows.<br>We apply ohe encoding on categorical features ['good\\_work\\_flg','Air\\_flg','car\\_type\\_flg','car\\_own\\_flg','gender\\_cd','home\\_address\\_cd','work\\_address\\_cd','SNA','first\\_time\\_cd','education\\_cd'].<br>We discretize and perform ohe encoding with continuous features ['region\\_rating', 'appl\\_rej\\_cnt','out\\_request\\_cnt','age','income','Score\\_bki'].<br>We use quantile-based discretization function.<br>We divide values into 3 quantiles. | (90,000, 45) | Default flag |\n",
        "|   |   |   |   |   |\n",
        "| 3 | [Zoo (Z)](https://archive.ics.uci.edu/dataset/111/zoo) | All features except 'legs' are binary.<br>We perform ohe encoding on 'legs' feature. | (101, 21) | Type 0 or 1 flag |\n",
        "|   |   |   |   |   |\n",
        "| 4 | [The Complete Pokemon Dataset (P)](https://www.kaggle.com/datasets/rounakbanik/pokemon) | Since the dataset is very wide, we select only 'agains\\_...' features and some generic like 'hp' and others.<br>We discretize and perform ohe encoding on all features.<br>We use quantile-based discretization function.<br>We divide all 'againts\\_...' features and generic features into 3 and 2 quantiles respectively. | (801, 50) | Legendary flag |\n"
      ],
      "metadata": {
        "id": "qLY8OhKVGCZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML\n",
        "\n",
        "First, using 'lazypredict' library we evaluated 30 classification models (see Tables 6,7,8 and 9). We see that there are good performing models that are not in the proposed list. For this reason we add 6 more models for classification.  \n",
        "\n",
        "We tune models' parameters using cross-validation (5 fold). Evaluation results of best models are presented in Table 10. In Table 11 we aggregated the results in pivot table. We see that all models have high scores for classical datasets (PP and Z) and struggle with close to real data (C and P). It is interesting to note that NearestCentroid model has highest score for C, but lower scores in all other datasets; LinearDiscriminantAnalysis model on average has highest among all datasets.\n",
        "\n",
        "Source code of preprocessing, models' fit and  evaluation is in following Jupyter notebook [hse.kamran.uz/osda23/fca/osda23\\_1.ipynb](https://hse.kamran.uz/osda23/fca/osda23_1.ipynb). All computations were performed on Apple M1 Pro, 32 GB."
      ],
      "metadata": {
        "id": "qX4i4eGoIWqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation results of best models of the cross-validation (5 fold) models’ parameter tuning. In bold best model’s highest F1 score\n",
        "\n",
        "<img src=\"https://hse.kamran.uz/osda23/fca/hw2_9.png\" alt=\"drawing\" width=\"400\"/>\n"
      ],
      "metadata": {
        "id": "MX9PkPVTHy0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pivot table of every model and dataset for F1 score.\n",
        "In bold highest F1 score. Second highest F1 score is underlined\n",
        "\n",
        "<img src=\"https://hse.kamran.uz/osda23/fca/hw2_10.png\" alt=\"drawing\" width=\"400\"/>\n"
      ],
      "metadata": {
        "id": "l1g-8dd4IK3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NeuralFCA Application"
      ],
      "metadata": {
        "id": "ifr8wherIfmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cuicenOBvdEQ",
        "outputId": "ea664122-1e09-4c57-9864-a1bc3d7df8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fcapy[all]\n",
            "  Downloading fcapy-0.1.4.3-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.9/162.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: fcapy 0.1.4.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (1.23.5)\n",
            "Collecting scikit-mine>=1 (from fcapy[all])\n",
            "  Downloading scikit_mine-1.0.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitarray>=2.5.1 (from fcapy[all])\n",
            "  Downloading bitarray-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (1.5.3)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (2.3.10)\n",
            "Collecting bitsets (from fcapy[all])\n",
            "  Downloading bitsets-0.8.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (1.10.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (3.2.1)\n",
            "Collecting caspailleur (from fcapy[all])\n",
            "  Downloading caspailleur-0.1.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from fcapy[all]) (7.7.1)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from scikit-mine>=1->fcapy[all]) (1.11.4)\n",
            "Collecting pyroaring>=0.3.4 (from scikit-mine>=1->fcapy[all])\n",
            "  Downloading pyroaring-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-mine>=1->fcapy[all]) (2.4.0)\n",
            "Collecting dataclasses>=0.6 (from scikit-mine>=1->fcapy[all])\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting wget>=3.2 (from scikit-mine>=1->fcapy[all])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from scikit-mine>=1->fcapy[all]) (0.20.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from scikit-mine>=1->fcapy[all]) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fcapy[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fcapy[all]) (2023.3.post1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->fcapy[all]) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fcapy[all]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->fcapy[all]) (4.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fcapy[all]) (3.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->fcapy[all]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->fcapy[all]) (6.3.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->fcapy[all])\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->fcapy[all]) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->fcapy[all]) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->fcapy[all]) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (3.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (5.5.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->fcapy[all]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->fcapy[all]) (0.2.12)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (4.1.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.15.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->fcapy[all]) (2.21)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=af8879dfcf5e13e38d18a8b1e573289bdb81333e24ffdf1e5c73e7cab39a79bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, pyroaring, dataclasses, bitarray, jedi, bitsets, scikit-mine, caspailleur, fcapy\n",
            "Successfully installed bitarray-2.9.1 bitsets-0.8.4 caspailleur-0.1.3 dataclasses-0.6 fcapy-0.1.4.3 jedi-0.19.1 pyroaring-0.4.4 scikit-mine-1.0.0 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (2.3.10)\n",
            "Collecting ipynb\n",
            "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: ipynb\n",
            "Successfully installed ipynb-0.5.1\n",
            "Collecting sparselinear\n",
            "  Downloading sparselinear-0.0.5-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sparselinear) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from sparselinear) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->sparselinear) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->sparselinear) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->sparselinear) (1.3.0)\n",
            "Installing collected packages: sparselinear\n",
            "Successfully installed sparselinear-0.0.5\n",
            "Requirement already satisfied: bitsets in /usr/local/lib/python3.10/dist-packages (0.8.4)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=495091 sha256=576a550091c49383c5fe5498956bf31bf4c65cd053453259f401c853f1b8097a\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=1035675 sha256=72e2fe703ea19897333d30c6e34d797bfaeff0f188808345f69c23dee2467bda\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.23.5)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=690648 sha256=a6695e39a0531e2179faa1d165e047eb5ca658d65c9f90732d5d794d10b72ed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-xxo9c0ed\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-xxo9c0ed\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 0e526ab546b135dd8d5fbd55174d74da1e4028be\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.4.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=1089899 sha256=37a317fcbf0cb1180e43416cd92923ccf518ea62c933056bd28012d4a49714c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tktm12q6/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "#@title install libs\n",
        "\n",
        "!pip install fcapy[all]\n",
        "!pip install frozendict\n",
        "!pip install ipynb\n",
        "!pip install sparselinear\n",
        "!pip install bitsets\n",
        "!pip install bitarray\n",
        "import torch\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cuda118.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y-UL4s6Ejr1",
        "outputId": "7bf59517-cfdf-46fb-c0c9-bdea908b5fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting neural_lib.py\n"
          ]
        }
      ],
      "source": [
        "# @title neural_lib.py\n",
        "%%writefile neural_lib.py\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, FrozenSet, Set, Dict\n",
        "import pandas as pd\n",
        "\n",
        "from fcapy.lattice import ConceptLattice\n",
        "from fcapy.lattice.formal_concept import FormalConcept\n",
        "from fcapy.poset import POSet\n",
        "from fcapy.visualizer.line_layouts import calc_levels\n",
        "\n",
        "import torch\n",
        "from sparselinear import SparseLinear\n",
        "\n",
        "\n",
        "@dataclass(eq=False)\n",
        "class DisjunctiveNeuron:\n",
        "    intent: FrozenSet[str]\n",
        "    level: int\n",
        "\n",
        "    def __eq__(self, other: 'DisjunctiveNeuron'):\n",
        "        return self.intent == other.intent and self.level == other.level\n",
        "\n",
        "    def __lt__(self, other: 'DisjunctiveNeuron'):\n",
        "        return self.intent & other.intent == other.intent and self.level > other.level\n",
        "\n",
        "    def __le__(self, other: 'DisjunctiveNeuron'):\n",
        "        return self < other or self == other\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.intent, self.level))\n",
        "\n",
        "\n",
        "class ConceptNetwork:\n",
        "    def __init__(self, poset: POSet, network=None, attributes: Tuple[str] = None, targets: Tuple[str] = None):\n",
        "        self._poset = poset\n",
        "        self._network = network\n",
        "        self._attributes = attributes\n",
        "        self._targets = targets\n",
        "\n",
        "    @property\n",
        "    def poset(self) -> POSet:\n",
        "        return self._poset\n",
        "\n",
        "    @property\n",
        "    def network(self) -> torch.nn.Sequential:\n",
        "        return self._network\n",
        "\n",
        "    @property\n",
        "    def attributes(self) -> Tuple[str]:\n",
        "        return self._attributes\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        return self._targets\n",
        "\n",
        "    def trace_description(self, description: FrozenSet[str], include_targets: bool = False) -> Set[int]:\n",
        "        P = self.poset\n",
        "\n",
        "        tops_activated = [node for node in P.tops if P[node].intent & description == P[node].intent]\n",
        "        activated_nodes = set(tops_activated)\n",
        "        for node in tops_activated:\n",
        "            activated_nodes |= P.descendants(node)\n",
        "        if not include_targets:\n",
        "            activated_nodes -= set(P.bottoms)\n",
        "\n",
        "        return activated_nodes\n",
        "\n",
        "    @classmethod\n",
        "    def from_lattice(\n",
        "            cls,\n",
        "            lattice: ConceptLattice, best_concepts_indices: List[int],\n",
        "            targets: Tuple[str]\n",
        "    ):\n",
        "        assert lattice.is_monotone, 'The lattice should be monotone'\n",
        "\n",
        "        targets = tuple(targets)\n",
        "\n",
        "        attrs_tpl = tuple(lattice[lattice.bottom].intent)\n",
        "        P = cls._poset_from_best_concepts(lattice[best_concepts_indices], targets, attrs_tpl)\n",
        "        P = cls._fill_levels(P)\n",
        "        return cls(P, None, attributes=attrs_tpl, targets=targets)\n",
        "\n",
        "    def fit(\n",
        "            self,\n",
        "            X_df: 'pd.DataFrame[bool]', y: 'pd.Series[bool]',\n",
        "            loss_fn=torch.nn.CrossEntropyLoss(), nonlinearity=torch.nn.ReLU,\n",
        "            n_epochs: int = 2000\n",
        "    ):\n",
        "        X = torch.tensor(X_df[list(self.attributes)].values).float()\n",
        "        y = torch.tensor(y.values).long()\n",
        "\n",
        "        self._network = self._poset_to_network(self.poset, nonlinearity)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.network.parameters())\n",
        "\n",
        "        for t in range(n_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.network(X)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def predict_proba(self, X_df: 'pd.DataFrame[bool]') -> torch.Tensor:\n",
        "        X = torch.tensor(X_df[list(self.attributes)].values).float()\n",
        "        return self.network(X)\n",
        "\n",
        "    def predict(self, X_df: 'pd.DataFrame[bool]') -> torch.Tensor:\n",
        "        return self.predict_proba(X_df).argmax(1)\n",
        "\n",
        "    def edge_weights_from_network(self) -> Dict[Tuple[int, int], float]:\n",
        "        max_level = self.poset[self.poset.bottoms[0]].level\n",
        "        nodes_per_levels = {lvl: [] for lvl in range(max_level + 1)}\n",
        "        for node_i, node in enumerate(self.poset):\n",
        "            nodes_per_levels[node.level].append(node_i)\n",
        "        nodes_per_levels = [nodes_per_levels[lvl] for lvl in range(max_level + 1)]\n",
        "\n",
        "        edge_weights = {}\n",
        "        for layer_i, nodes in enumerate(nodes_per_levels[:-1]):\n",
        "            next_nodes = nodes_per_levels[layer_i+1]\n",
        "\n",
        "            nn_layer = self.network[layer_i*2]\n",
        "            idxs = nn_layer.weight.indices().numpy().T.tolist()\n",
        "            vals = nn_layer.weight.values().numpy()\n",
        "\n",
        "            for (child_i, parent_i), v in zip(idxs, vals):\n",
        "                edge_weights[(nodes[parent_i], next_nodes[child_i])] = v\n",
        "        return edge_weights\n",
        "\n",
        "    @staticmethod\n",
        "    def _poset_from_best_concepts(\n",
        "            best_concepts: List[FormalConcept], targets: Tuple[str], attrs_tpl: Tuple[str]\n",
        "    ) -> POSet:\n",
        "        P_best = POSet(best_concepts)\n",
        "        lvls = calc_levels(P_best)[0]\n",
        "        lvls = [lvl + 1 for lvl in lvls]\n",
        "        target_lvl = max(lvls) + 1\n",
        "\n",
        "        attrs_set = set(attrs_tpl)\n",
        "\n",
        "        best_neurons = [DisjunctiveNeuron(frozenset(c.intent), lvl) for c, lvl in zip(P_best, lvls)]\n",
        "        first_level_neurons = [DisjunctiveNeuron(frozenset({m}), 0) for m in attrs_tpl]\n",
        "        last_level_neurons = [DisjunctiveNeuron(frozenset({f\"y={y}\"} | attrs_set), target_lvl) for y in targets]\n",
        "        return POSet(first_level_neurons + best_neurons + last_level_neurons)\n",
        "\n",
        "    @staticmethod\n",
        "    def _fill_levels(poset: POSet) -> POSet:\n",
        "        nodes_i = sorted(range(len(poset)), key=lambda node_i: poset[node_i].level)\n",
        "        for node_i in nodes_i:\n",
        "            children_i = poset.children(node_i)\n",
        "            if len(children_i) == 0:\n",
        "                continue\n",
        "\n",
        "            max_children_level = max([poset[child_i].level for child_i in children_i])\n",
        "            for lvl in range(poset[node_i].level+1, max_children_level):\n",
        "                poset.add(DisjunctiveNeuron(poset[node_i].intent, lvl))\n",
        "        return poset\n",
        "\n",
        "    @staticmethod\n",
        "    def _poset_to_network(poset: POSet, nonlinearity: type = torch.nn.ReLU) -> 'torch.nn.Sequential':\n",
        "        max_level = poset[poset.bottoms[0]].level\n",
        "        nodes_per_levels = {lvl: [] for lvl in range(max_level + 1)}\n",
        "        for node_i, node in enumerate(poset):\n",
        "            nodes_per_levels[node.level].append(node_i)\n",
        "        nodes_per_levels = [nodes_per_levels[lvl] for lvl in range(max_level + 1)]\n",
        "\n",
        "        connectivities = []\n",
        "        for layer_i, layer in enumerate(nodes_per_levels[1:]):\n",
        "            layer_i += 1\n",
        "            prev_layer = nodes_per_levels[layer_i - 1]\n",
        "            layer_con = [(layer.index(node), prev_layer.index(parent))\n",
        "                         for node in layer for parent in poset.parents(node)]\n",
        "            connectivities.append(layer_con)\n",
        "\n",
        "        linear_layers = []\n",
        "        for layer_i in range(max_level):\n",
        "            con = torch.tensor(connectivities[layer_i]).T\n",
        "            layer = SparseLinear(len(nodes_per_levels[layer_i]), len(nodes_per_levels[layer_i + 1]), connectivity=con)\n",
        "            linear_layers.append(layer)\n",
        "\n",
        "        layers = [layer for ll in linear_layers for layer in [ll, nonlinearity()]][:-1] + [torch.nn.Softmax(dim=1)]\n",
        "        model_sparse = torch.nn.Sequential(*layers)\n",
        "        return model_sparse\n",
        "\n",
        "\n",
        "def neuron_label_func(el_i: int, P: POSet, M: set, only_new_attrs: bool = True):\n",
        "    el = P[el_i]\n",
        "\n",
        "    if len(el.intent - M) > 0:  # if target node\n",
        "        attrs_to_show = list(el.intent - M)\n",
        "    else:\n",
        "        attrs_to_show = set(el.intent)\n",
        "        if only_new_attrs:\n",
        "            for parent_i in P.parents(el_i):\n",
        "                attrs_to_show = attrs_to_show - P[parent_i].intent\n",
        "\n",
        "        attrs_to_show = list(attrs_to_show)\n",
        "    return ','.join(attrs_to_show)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "cellView": "form",
        "id": "1VQyD97tGPuk"
      },
      "outputs": [],
      "source": [
        "#@title import libs\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, jaccard_score, recall_score, accuracy_score, classification_report\n",
        "\n",
        "from fcapy.context import FormalContext\n",
        "from fcapy.lattice import ConceptLattice\n",
        "\n",
        "from fcapy.visualizer import LineVizNx\n",
        "import neural_lib as nl\n",
        "from fcapy.utils.utils import powerset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.facecolor'] = (1,1,1,1)\n",
        "\n",
        "\n",
        "from fcapy import LIB_INSTALLED\n",
        "if LIB_INSTALLED['numpy']:\n",
        "    import numpy as np\n",
        "\n",
        "from sparselinear import SparseLinear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hB6gJ09fFmi9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title import data\n",
        "\n",
        "def get(path):\n",
        "    # Using the requests library to handle the URL\n",
        "    try:\n",
        "        response = requests.get(path)\n",
        "        response.raise_for_status()  # This will raise an HTTPError if the HTTP request returned an unsuccessful status code.\n",
        "\n",
        "        # Reading the content of the file into a pandas DataFrame\n",
        "        df = pd.read_csv(StringIO(response.text)).astype(bool)\n",
        "\n",
        "    except requests.exceptions.HTTPError as errh:\n",
        "        print(\"Http Error:\", errh)\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(\"Error Connecting:\", errc)\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(\"Timeout Error:\", errt)\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(\"Oops: Something Else\", err)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"No data: Empty Data Received\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "    return df\n",
        "\n",
        "root = 'https://hse.kamran.uz/osda23/fca/'\n",
        "\n",
        "# root = ''\n",
        "\n",
        "pp = get(f'{root}prepared_pp.csv')\n",
        "c  = get(f'{root}prepared_c.csv')\n",
        "z  = get(f'{root}prepared_z.csv')\n",
        "p  = get(f'{root}prepared_p.csv')\n",
        "\n",
        "\n",
        "y_pp = get(f'{root}target_pp.csv')\n",
        "y_c  = get(f'{root}target_c.csv')\n",
        "y_z  = get(f'{root}target_z.csv')\n",
        "y_p  = get(f'{root}target_p.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train | f1_score\n",
        "\n",
        "# DELTA_STABILITY\n",
        "from numpy.random import seed as np_seed\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "def log_stability_lbound(c_i, lattice: ConceptLattice, n_bin_attrs: int) -> float:\n",
        "    extent_i = set(lattice[c_i].extent_i)\n",
        "    children_i = lattice.children(c_i)\n",
        "    if children_i:\n",
        "        bound = min(len(extent_i - set(lattice[child_i].extent_i)) for child_i in children_i)\n",
        "    else:\n",
        "        bound = math.inf\n",
        "    bound -= math.log2(n_bin_attrs)\n",
        "    return bound\n",
        "\n",
        "def delta_stability(c_i, lattice: ConceptLattice, n_bin_attrs: int) -> float:\n",
        "    return log_stability_lbound(c_i, lattice, n_bin_attrs)+math.log2(n_bin_attrs)\n",
        "\n",
        "\n",
        "\n",
        "def get_train_test(X, y):\n",
        "    X.index = X.index.astype('str')\n",
        "    y.index = y.index.astype('str')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=RANDOM_SEED,\n",
        "                                                        shuffle=True,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X = [\n",
        "    pp,\n",
        "     z,\n",
        "     p,\n",
        "     c,\n",
        "     ]\n",
        "y = [\n",
        "    y_pp,\n",
        "     y_z,\n",
        "     y_p,\n",
        "     y_c,\n",
        "     ]\n",
        "models = []\n",
        "for X_, y_ in tqdm(zip(X,y)):\n",
        "    m = []\n",
        "    # Split the data to train and test\n",
        "    X_train, X_test, y_train, y_test = get_train_test(X_, y_)\n",
        "\n",
        "    # Put binarized data in FormalContext and compute monotone ConceptLattice\n",
        "    K_train = FormalContext(data = X_train.values, target=y_train.values, attribute_names=X_train.columns)\n",
        "    L = ConceptLattice.from_context(K_train, algo='Sofia', is_monotone=True)\n",
        "\n",
        "    # Compute F1 score for each formal concept (assuming that an object is predicted True if it is in the extent of the concept)\n",
        "    for i in range(len(L)):\n",
        "        y_preds = np.zeros(K_train.n_objects)\n",
        "        y_preds[list(L[i].extent_i)] = 1\n",
        "        L[i].measures['f1_score']=f1_score(y_train, y_preds)\n",
        "\n",
        "    # Select indices of the best concepts from the lattice\n",
        "    best_concepts = list(L.measures['f1_score'].argsort()[::-1])\n",
        "    for i in range(len(best_concepts)):\n",
        "        if len({g_i for c in L[best_concepts[:i]] for g_i in c.extent_i})==K_train.n_objects:\n",
        "            print()\n",
        "            print('Number of best concepts to cover all objects: ',i)\n",
        "            best_concepts = best_concepts[:i]\n",
        "            break\n",
        "\n",
        "    # Construct neural network based on concept lattice\n",
        "    cn = nl.ConceptNetwork.from_lattice(L, best_concepts, sorted(set(y_train.iloc[:,0])))\n",
        "    cn.fit(X_train, y_train.iloc[:,0],  n_epochs =1000)\n",
        "    y_pred = cn.predict(X_test).numpy()\n",
        "    print()\n",
        "    print('Recall score:', recall_score(y_test.values.astype('int'), y_pred))\n",
        "    print('F1     score:', f1_score(y_test.values.astype('int'), y_pred))\n",
        "    print('Accuracy score:', accuracy_score(y_test.values.astype('int'), y_pred))\n",
        "    print()\n",
        "    models.append([cn, K_train, L, [X_train, X_test, y_train, y_test]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wB2D_VwiIKHs",
        "outputId": "c739c8e9-e5f1-40f8-b46c-081035399a27"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of best concepts to cover all objects:  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:03,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.0\n",
            "F1     score: 0.0\n",
            "Accuracy score: 0.8\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:08,  4.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 1.0\n",
            "F1     score: 0.7777777777777778\n",
            "Accuracy score: 0.6363636363636364\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:14,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.0\n",
            "F1     score: 0.0\n",
            "Accuracy score: 0.9135802469135802\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [07:20, 110.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.0\n",
            "F1     score: 0.0\n",
            "Accuracy score: 0.8667777777777778\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "cellView": "form",
        "id": "TCZQjXseP6Nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7f93d3-5b8a-4cad-de57-b91605333322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of best concepts to cover all objects:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:01,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.8571428571428571\n",
            "F1     score: 0.923076923076923\n",
            "Accuracy score: 0.9714285714285714\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:03,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 1.0\n",
            "F1     score: 1.0\n",
            "Accuracy score: 1.0\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:05,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.0\n",
            "F1     score: 0.0\n",
            "Accuracy score: 0.9135802469135802\n",
            "\n",
            "\n",
            "Number of best concepts to cover all objects:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [01:47, 26.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recall score: 0.0\n",
            "F1     score: 0.0\n",
            "Accuracy score: 0.8667777777777778\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@title train | delta_stability\n",
        "\n",
        "# DELTA_STABILITY\n",
        "from numpy.random import seed as np_seed\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "def log_stability_lbound(c_i, lattice: ConceptLattice, n_bin_attrs: int) -> float:\n",
        "    extent_i = set(lattice[c_i].extent_i)\n",
        "    children_i = lattice.children(c_i)\n",
        "    if children_i:\n",
        "        bound = min(len(extent_i - set(lattice[child_i].extent_i)) for child_i in children_i)\n",
        "    else:\n",
        "        bound = math.inf\n",
        "    bound -= math.log2(n_bin_attrs)\n",
        "    return bound\n",
        "\n",
        "def delta_stability(c_i, lattice: ConceptLattice, n_bin_attrs: int) -> float:\n",
        "    return log_stability_lbound(c_i, lattice, n_bin_attrs)+math.log2(n_bin_attrs)\n",
        "\n",
        "\n",
        "\n",
        "def get_train_test(X, y):\n",
        "    X.index = X.index.astype('str')\n",
        "    y.index = y.index.astype('str')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                        test_size=0.1,\n",
        "                                                        random_state=RANDOM_SEED,\n",
        "                                                        shuffle=True,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X = [\n",
        "    pp,\n",
        "     z,\n",
        "     p,\n",
        "     c,\n",
        "     ]\n",
        "y = [\n",
        "    y_pp,\n",
        "     y_z,\n",
        "     y_p,\n",
        "     y_c,\n",
        "     ]\n",
        "models = []\n",
        "for X_, y_ in tqdm(zip(X,y)):\n",
        "    m = []\n",
        "    # Split the data to train and test\n",
        "    X_train, X_test, y_train, y_test = get_train_test(X_, y_)\n",
        "\n",
        "    # Put binarized data in FormalContext and compute monotone ConceptLattice\n",
        "    K_train = FormalContext(data = X_train.values, target=y_train.values, attribute_names=X_train.columns)\n",
        "    L = ConceptLattice.from_context(K_train, algo='Sofia', is_monotone=True)\n",
        "\n",
        "    # Compute F1 score for each formal concept (assuming that an object is predicted True if it is in the extent of the concept)\n",
        "    for i in range(len(L)):\n",
        "        y_preds = np.zeros(K_train.n_objects)\n",
        "        y_preds[list(L[i].extent_i)] = 1\n",
        "        L[i].measures['delta_stability'] = delta_stability(i, L, X_train.shape[1])\n",
        "\n",
        "    # Select indices of the best concepts from the lattice\n",
        "    best_concepts = list(L.measures['delta_stability'].argsort()[::-1])\n",
        "    for i in range(len(best_concepts)):\n",
        "        if len({g_i for c in L[best_concepts[:i]] for g_i in c.extent_i})==K_train.n_objects:\n",
        "            print()\n",
        "            print('Number of best concepts to cover all objects: ',i)\n",
        "            best_concepts = best_concepts[:i]\n",
        "            break\n",
        "\n",
        "    # Construct neural network based on concept lattice\n",
        "    cn = nl.ConceptNetwork.from_lattice(L, best_concepts, sorted(set(y_train.iloc[:,0])))\n",
        "    cn.fit(X_train, y_train.iloc[:,0],  n_epochs =1000)\n",
        "    y_pred = cn.predict(X_test).numpy()\n",
        "    print()\n",
        "    print('Recall score:', recall_score(y_test.values.astype('int'), y_pred))\n",
        "    print('F1     score:', f1_score(y_test.values.astype('int'), y_pred))\n",
        "    print('Accuracy score:', accuracy_score(y_test.values.astype('int'), y_pred))\n",
        "    print()\n",
        "    models.append([cn, K_train, L, [X_train, X_test, y_train, y_test]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We applied NeuralFCA to 4 differnet datasets. We used F1 score and Delta stability metrics to for finding best concepts for ConceptNetwork learning. We see that version with delta stability gives better results\n",
        "\n",
        "For difficult datasets P and C we got unsatisfactory results (F1 scores 0 and 0 vs 0.33 and 0.66 in baseline respectivelly). We believe that this is consequence of inherit inconsistency of manmade system of Pokemons and chaotic nature of credit defaults\n",
        "\n",
        "We got high results for PP and Z datasets (F1 scores 0.92 and 1 vs 1 and 1 in basesline respectively). We believe that this is consequence of systemic nature of the wild life features"
      ],
      "metadata": {
        "id": "9Vct2cY7BVOc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}